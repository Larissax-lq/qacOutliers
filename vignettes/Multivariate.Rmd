---
title: "Multivariate"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{multiOutliers}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(qacOutliers)
```

## What are multivariate outliers? How do you detect them? 

A multivariate outlier is an outlier that can only be detected by looking at two variables in combination. The graph below shows examples of multivariate outliers. The data for this graph is taken from the `Salaries` dataset from the `carData` package.

```{r, echo = F, message = F, warning = F}
data(Salaries, package="carData")
#getting the outliers
results <- multiOutliers(Salaries, method = "LoF")
row <- as.numeric(results$Row)

library(tidyverse)
#getting just a subset for red points
subset <- Salaries %>% 
  slice(row)

#getting the one point for labelling
point <- Salaries %>% 
  filter(yrs.since.phd > 20 & yrs.since.phd < 30 & salary < 70000)

#graphing
ggplot()+
  geom_point(data = Salaries, aes(x=yrs.since.phd, y=salary))+
  geom_point(data = subset, aes(x=yrs.since.phd, y = salary), color = "red")+
  geom_label(data = point, aes(x=yrs.since.phd, y=salary), label = rownames(point))+
  labs(title = "Years since PhD and salary (USD)", x = "Years since PhD", y = "Salary (USD)")+
  scale_y_continuous(labels = scales::dollar)+
  theme_minimal()
```

All of the red dots are multivariate outliers. The point labelled 1 on the graph is a clear example of a multivariate outlier. This person has had 22 years since their PhD, a normal value for that variable, and makes `$62,884` dollars, which is also a normal value for salary. However, when combining these two features, a person who has had 22 years since their PhD and makes only `$62,884` is making much less than other professors within their experience range. 

The outliers in this graph were detected using the LoF method, and more detail about that method can be provided below. This package specifically focuses on four different methods for finding multivariate outliers: kNN, LoF, mahalanobis distance, and iForest. 

## kNN

kNN calculates the distances between a data point and its k-nearest neighbors and assigns an outlier score based on that distance. The principle that guides kNN is that outliers lay far away from their neighbours, so each of the distances is interpreted within that context. Because some variables in the data may have much larger ranges that others (ex. a variable has a range from 1-10 and another has a range of -10000 to 10000), the data is standardized before calculating the distances. 

Here is an example of the distances for the first 5 rows in `mtcarsOutliers`, a dataset included with this package. 

```{r, echo = F, message = F, warning = F}
mtcarsOutliers_st <- scale(mtcarsOutliers[-1])
knn_distances <- FNN::knn.dist(mtcarsOutliers_st, k=5)
head(knn_distances, 5)
```

After each of these distances are calculated, the average for each row is calculated. Here are the average scores for the 5 rows shown above. This step is why it's important to standardize the data before finding the distances. 

```{r, echo = F, message = F, warning = F}
avg_knn_distances <- rowMeans(knn_distances)
head(avg_knn_distances, 5)
```

In this function, the next step involves creating a threshold for declaring a point an outlier. To calculate this threshold, the function takes the average of each row (after that row's average has been calculated), and adds 2 times the standard deviation of each row to that number. In this case, the threshold is the number below. 

```{r, echo = F, message = F, warning = F}
threshold <- mean(avg_knn_distances) + 2 * sd(avg_knn_distances)
threshold
```

Outliers are considered any points with a score above the calculated threshold. In this case, the outliers are shown below. 

```{r, echo = F, message = F, warning = F}
multiOutliers(mtcarsOutliers, method="kNN")
```

### Customizing the k parameter

The value `k` tells the function how many points to consider as neighbors when identifying distances between each of the points. The default value, 5, finds the distance between each point the 5 points that are closest to that point. The choice of `k` significantly impacts the results, and smaller values are generally more sensitive to outliers. You can supply your own value of `k`, which may change the results of the function.

```{r}
multiOutliers(mtcarsOutliers, method = "kNN", k = 10)
```

### Example Output
When using the kNN method with the default `k=5`, the function returns:

- Method: "kNN", indicating the method used.
- Data: The dataset name.
- Variables: The numeric columns considered for outlier detection.
- Row: Indices of rows identified as outliers.
- Score: Average kNN distance scores of detected outliers.
- Message: A summary message indicating whether outliers were detected.
- k: The number of nearest neighbors considered.

```{r, echo = F, message = F}
result <- multiOutliers(mtcarsOutliers, method = "kNN")
print(result)
```

### Notes and Considerations

1. Numeric Data Only: The kNN method requires numeric variables. Non-numeric columns are automatically excluded.

2. Robustness: kNN does not assume a specific distribution of data, so it is robust to non-normality, making it a better tool to handle non-normal data than other outlier detection methods. 

To learn more about kNN and how it's used in multivariate outlier detection, visit these resources: 
[GeeksforGeeks.com]: https://www.geeksforgeeks.org/k-nearest-neighbours/#
[Dualitytech.com]: https://dualitytech.com/blog/anomaly-detection-k-nearest-neighbors/
[StatQuest]: https://www.youtube.com/watch?v=HVXime0nQeI

### Graphical output

Here is a graphical representation of the outliers shown above. 

```{r}
#add plot function when it's done
```


## Local outlier factor (LoF)

LoF for a point is the average density around the k-nearest neighbors of the point divided by the density around the point itself. If the  LoF score is above 1, it ts more likely to be anomalous, if it is below 1, it is less likely to be anomalous. 

-describe how this works within our function

```{r}
multiOutliers(mtcarsOutliers, method="LoF")
```

-how to customize results (change threshold and stuff)
-how to intrepret results

## Mahalanobis 

The Mahalanobis distance measures the distance of a point from the center of a multivariate distribution while accounting for the correlation between variables. This method identifies outliers by calculating how far each point is from the data's multivariate mean, considering the covariance matrix of the data. This approach is particularly useful when variables are highly correlated or have different scales.

Before using the Mahalanobis distance, the function automatically selects numeric columns from the dataset. Non-numeric variables are excluded, ensuring compatibility with the method. The distances are then calculated using the `outliers_mahalanobis` function from the `Routliers` package.

Here is an example of calculating Mahalanobis distances for the `mtcarsOutliers` dataset included with this package:

```{r, echo = F, message = F}
library(Routliers)
mtcarsOutliers_numeric <- mtcarsOutliers[-1]  # Remove non-numeric column
results <- outliers_mahalanobis(as.matrix(mtcarsOutliers_numeric), alpha = 0.05)

# Display Mahalanobis distances for the first 5 rows
head(results$dist_from_center, 5)
```

The outliers are identified by the function and their indices are returned:
```{r, echo = F, message = F}
results$outliers_pos
```

Outliers are identified by comparing the Mahalanobis distance of each point to a threshold derived from the chi-squared distribution. Points with distances greater than the critical value at a specified significance level (`alpha`) are flagged as outliers. The default `alpha` is 0.05, which corresponds to a 95% confidence level. You can customize this value to adjust the sensitivity of the detection.

Here is the threshold for the dataset using the default `alpha = 0.05`:
```{r, echo = F, message = F}
alpha <- 0.05
threshold <- qchisq(1 - alpha, df = ncol(mtcarsOutliers_st))
threshold
```

The outliers identified are shown below:
```{r}
multiOutliers(mtcarsOutliers, method = "mahalanobis")
```

### Customizing the alpha parameter

The `alpha` parameter in outliers_mahalanobis determines the significance level for outlier detection. Lower values (e.g., `alpha = 0.01`) result in stricter thresholds, identifying fewer points as outliers. You can modify `alpha` as follows:
```{r}
multiOutliers(mtcarsOutliers, method = "mahalanobis", alpha = 0.01)
```

### Example Output
When using the Mahalanobis method with the default `alpha = 0.05`, the function returns:

- Method: "mahalanobis", indicating the method used.
- Data: The dataset name.
- Variables: The numeric columns considered.
- Row: Indices of rows identified as outliers.
- Score: Mahalanobis distance scores of detected outliers.
- Message: A summary message indicating whether outliers were detected.
- Alpha: The significance level used.

```{r, echo = F, message = F}
result <- multiOutliers(mtcarsOutliers, method = "mahalanobis", alpha = 0.05)
print(result)
```

### Notes and Considerations

1. Numeric Data Only: The Mahalanobis method requires numeric variables. Non-numeric columns are automatically excluded.

2. Multivariate Normality: This method assumes the data follows a multivariate normal distribution. Deviations from normality or the presence of extreme outliers may affect the results.

To learn more about Mahalanobis distance and how it's used in multivariate outlier detection, visit these resources: 
[Statisticshowto.com]: https://www.statisticshowto.com/mahalanobis-distance/
[Builtin.com]: https://builtin.com/data-science/mahalanobis-distance

## iForest 

iForest stands for isolation forest. First, it randomly selects a variable, then randomly selects a value of that variable. This will work for both quantitaive and categorical; if the variable is quantitative, it will randomly pick a number in the range of the variable, and if the variable is categorical it will randomly pick a level. Then it will split the data using the value randomly selected eariler. 

The iForest method repeats the above steps until all points are separately in their own node. Then, for each data point, it counts how many splits were needed to isolate it. 

Because the selection of variables and values is random, this process will return different results each time. Therefore, isolation trees are repeated many times and the results are averaged over all trials. More isolated points will have lower average path lengths. They are more isolated from the rest of the data's distribution, therefore they are called outliers. 

```{r}
multiOutliers(mtcarsOutliers, method="iForest")
```


